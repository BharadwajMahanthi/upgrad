{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset from Kaggle\n",
    "# Run the below shell commands in a notebook or Colab environment for Kaggle setup\n",
    "!mkdir -p ~/.kaggle  # Ensure the directory exists\n",
    "!cp kaggle.json ~/.kaggle/  # Copy the API key file to the correct location\n",
    "!chmod 600 ~/.kaggle/kaggle.json  # Set permissions for the file\n",
    "!kaggle datasets download amerzishminha/forest-fire-smoke-and-non-fire-image-dataset --unzip  # Download and unzip the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the downloaded and unzipped dataset directory\n",
    "dataset_dir = './forest-fire-smoke-and-non-fire-image-dataset/train'  # Change this path to reflect your dataset structure after unzipping\n",
    "\n",
    "# Initialize a dictionary to store file extensions and their counts\n",
    "file_types = {}\n",
    "\n",
    "# Traverse through the dataset directory and count file extensions\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        # Get the file extension\n",
    "        file_extension = os.path.splitext(file)[1].lower()  # Use lower to avoid case sensitivity\n",
    "        if file_extension in file_types:\n",
    "            file_types[file_extension] += 1\n",
    "        else:\n",
    "            file_types[file_extension] = 1\n",
    "\n",
    "# Convert the result to a DataFrame for better visualization\n",
    "file_types_df = pd.DataFrame(list(file_types.items()), columns=['File Extension', 'Count'])\n",
    "file_types_df = file_types_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Print the result\n",
    "print(file_types_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Directories for images\n",
    "train_dir = './forest-fire-smoke-and-non-fire-image-dataset/train'\n",
    "\n",
    "# Function to get image counts by class\n",
    "def get_image_counts(directory):\n",
    "    class_counts = {}\n",
    "    for subdir in os.listdir(directory):\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            class_counts[subdir] = len([f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))])\n",
    "    return class_counts\n",
    "\n",
    "# Function to get image dimensions distribution\n",
    "def get_image_dimensions(directory):\n",
    "    dimensions = []\n",
    "    for subdir in os.listdir(directory):\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for file in os.listdir(subdir_path):\n",
    "                file_path = os.path.join(subdir_path, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    try:\n",
    "                        with Image.open(file_path) as img:\n",
    "                            dimensions.append(img.size)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "    return dimensions\n",
    "\n",
    "# Get class counts\n",
    "class_counts = get_image_counts(train_dir)\n",
    "\n",
    "# Get image dimensions\n",
    "image_dimensions = get_image_dimensions(train_dir)\n",
    "\n",
    "# Plot the class distribution\n",
    "def plot_class_distribution(class_counts):\n",
    "    class_names = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(class_names, counts, color='skyblue')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the image dimensions distribution\n",
    "def plot_image_dimensions(image_dimensions):\n",
    "    df = pd.DataFrame(image_dimensions, columns=['Width', 'Height'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(df['Width'], bins=30, alpha=0.5, label='Width', color='blue')\n",
    "    plt.hist(df['Height'], bins=30, alpha=0.5, label='Height', color='green')\n",
    "    plt.title('Image Dimensions Distribution')\n",
    "    plt.xlabel('Pixels')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot class distribution and image dimensions distribution\n",
    "plot_class_distribution(class_counts)\n",
    "plot_image_dimensions(image_dimensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, LSTM, GRU, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras import mixed_precision, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import keras_tuner as kt  # Import Keras Tuner for hyperparameter tuning\n",
    "from PIL import Image\n",
    "\n",
    "# Enable device placement logging to verify that operations are running on the GPU\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Set the device to GPU explicitly to ensure TensorFlow uses the GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Check if GPU is available and enable memory growth to avoid allocating all memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth enabled for\", gpus)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error setting memory growth on GPU:\", e)\n",
    "\n",
    "# Enable mixed precision policy\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Set directories for training and validation\n",
    "base_dir = './forest-fire-smoke-and-non-fire-image-dataset/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "\n",
    "def count_images(root_dir):\n",
    "    total_images = 0\n",
    "    for base, dirs, files in os.walk(root_dir):\n",
    "        if files:\n",
    "            print(f\"Folder {base} contains {len(files)} images\")\n",
    "            total_images += len(files)\n",
    "    return total_images\n",
    "\n",
    "train_images = count_images(train_dir)\n",
    "test_images = count_images(test_dir)\n",
    "\n",
    "print(f\"Total images in train: {train_images}\")\n",
    "print(f\"Total images in test: {test_images}\")\n",
    "\n",
    "\n",
    "def convert_images(root_dir):\n",
    "    count = 0\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Convert image to RGB and save as JPG\n",
    "                    rgb_img = img.convert('RGB')\n",
    "                    new_file_path = os.path.splitext(file_path)[0] + '.jpg'\n",
    "                    rgb_img.save(new_file_path, 'JPEG')\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Processed {count} images.\")\n",
    "\n",
    "# Convert images in train and test directories\n",
    "convert_images(train_dir)\n",
    "convert_images(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "classes = ['non fire', 'fire', 'Smoke']\n",
    "\n",
    "# Data augmentation and loading using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    brightness_range=[0.2, 1.0],  # Added brightness augmentation\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    rescale=1.0 / 255.0,\n",
    "    validation_split=0.1  # 10% validation split\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# Flow from directory to load images batch-wise\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Do not shuffle the test set\n",
    ")\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the model building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    base_model.trainable = False  # Freeze layers initially\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(classes), activation='softmax', dtype='float32')  # Ensuring softmax layer is float32 for stability\n",
    "    ])\n",
    "\n",
    "    # Hyperparameter tuning for learning rate\n",
    "    lr = hp.Float('learning_rate', min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "\n",
    "    # Optimizer setup\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,  # Number of models to try\n",
    "    executions_per_trial=1,  # Number of times to repeat each model\n",
    "    directory='kt_search',  # Directory to store the tuner results\n",
    "    project_name='fire_detection_tuning'\n",
    ")\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\", verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(train_generator, validation_data=validation_generator, epochs=10, callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model with early stopping and dynamic learning rate\n",
    "H = best_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20,  # More epochs to let early stopping find the best one\n",
    "    steps_per_epoch=train_generator.samples // 32,\n",
    "    validation_steps=validation_generator.samples // 32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Saving the best model\n",
    "best_model_output_path = 'output/fire_detection_best_model.h5'\n",
    "print(f\"[INFO] serializing best model to '{best_model_output_path}'...\")\n",
    "best_model.save(best_model_output_path)\n",
    "\n",
    "# Plotting loss and accuracy\n",
    "N = np.arange(0, len(H.history[\"loss\"]))\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Losses\")\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Accuracies\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"output/training_plot.png\")\n",
    "\n",
    "# Load the best model for testing\n",
    "print(\"[INFO] loading best model...\")\n",
    "best_model = load_model(best_model_output_path)\n",
    "\n",
    "# Test the model and evaluate\n",
    "predictions = best_model.predict(test_generator)\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(\"output/confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n",
    "\n",
    "# Test the model on random samples and save the results\n",
    "for i in range(10):\n",
    "    img, label = test_generator.next()\n",
    "    org_img = img[0] * 255.0\n",
    "    pred = best_model.predict(np.expand_dims(img[0], axis=0))[0]\n",
    "    result = classes[np.argmax(pred)]\n",
    "    org_img = cv2.resize(org_img, (500, 500))\n",
    "    cv2.putText(org_img, result, (35, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 3)\n",
    "    cv2.imwrite(f'output/testing/{i}.png', org_img)\n",
    "\n",
    "# Clear session to free memory\n",
    "K.clear_session()\n",
    "# Load the best model for testing\n",
    "print(\"[INFO] loading best model...\")\n",
    "model = load_model('output/fire_detection_best_model.h5')\n",
    "\n",
    "# Prepare test data using ImageDataGenerator\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important for consistency in evaluation\n",
    ")\n",
    "\n",
    "# Predict using the test data\n",
    "print(\"[INFO] evaluating model...\")\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "print(f\"Test Loss: {test_loss:.2f}\")\n",
    "\n",
    "# Generate predictions for further analysis like confusion matrix\n",
    "predictions = model.predict(test_generator)\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
