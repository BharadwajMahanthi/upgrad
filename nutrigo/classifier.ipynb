{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:29:46,591:INFO:1 Physical GPU(s), 1 Logical GPU(s) found.\n",
      "2024-09-23 23:29:46,594:INFO:Loading Core Recipe Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU(s), 1 Logical GPU(s) found.\n",
      "Loading Core Recipe Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:29:48,734:INFO:Core Recipes Shape: (45630, 6)\n",
      "2024-09-23 23:29:48,735:INFO:Loading Raw Recipe Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Recipes Shape: (45630, 6)\n",
      "Loading Raw Recipe Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:30:09,176:INFO:Raw Recipes Shape: (49698, 9)\n",
      "2024-09-23 23:30:09,177:INFO:Loading Interaction Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Recipes Shape: (49698, 9)\n",
      "Loading Interaction Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:30:14,067:INFO:Handling Missing Values...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Missing Values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:30:14,533:INFO:Missing values handled.\n",
      "2024-09-23 23:30:14,534:INFO:Encoding Categorical Variables...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled.\n",
      "Encoding Categorical Variables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:30:31,828:INFO:Categorical variables encoded.\n",
      "2024-09-23 23:30:31,829:INFO:Preprocessing Text Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables encoded.\n",
      "Preprocessing Text Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mbpd1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mbpd1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-09-23 23:31:05,531:INFO:Text data preprocessed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data preprocessed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:31:06,046:INFO:Label encoders saved.\n",
      "2024-09-23 23:31:06,047:INFO:Parsing 'nutritions' column with nested dictionary structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoders saved.\n",
      "Parsing 'nutritions' column with nested dictionary structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:31:40,684:INFO:Columns after parsing 'nutritions': ['recipe_id', 'recipe_name', 'image_url', 'ingredients', 'cooking_directions', 'recipe_id_encoded', 'ingredients_clean', 'calories', 'protein', 'fat', 'carbohydrates', 'fiber']\n",
      "2024-09-23 23:31:40,685:INFO:Parsing 'nutritions' column completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after parsing 'nutritions':\n",
      "['recipe_id', 'recipe_name', 'image_url', 'ingredients', 'cooking_directions', 'recipe_id_encoded', 'ingredients_clean', 'calories', 'protein', 'fat', 'carbohydrates', 'fiber']\n",
      "Parsing 'nutritions' column completed.\n",
      "Parsing 'nutritions' column with nested dictionary structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:31:41,140:INFO:Parsing 'nutritions' column with nested dictionary structure...\n",
      "2024-09-23 23:32:17,842:INFO:Columns after parsing 'nutritions': ['recipe_id', 'recipe_name', 'aver_rate', 'image_url', 'review_nums', 'ingredients', 'cooking_directions', 'reviews', 'recipe_id_encoded', 'ingredients_clean', 'calories', 'protein', 'fat', 'carbohydrates', 'fiber']\n",
      "2024-09-23 23:32:17,843:INFO:Parsing 'nutritions' column completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after parsing 'nutritions':\n",
      "['recipe_id', 'recipe_name', 'aver_rate', 'image_url', 'review_nums', 'ingredients', 'cooking_directions', 'reviews', 'recipe_id_encoded', 'ingredients_clean', 'calories', 'protein', 'fat', 'carbohydrates', 'fiber']\n",
      "Parsing 'nutritions' column completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:18,283:INFO:Vectorizing Ingredients with TF-IDF...\n",
      "2024-09-23 23:32:18,284:INFO:Loading TF-IDF Vectorizer from tfidf_vectorizer.pkl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Ingredients with TF-IDF...\n",
      "Loading TF-IDF Vectorizer from tfidf_vectorizer.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:19,402:INFO:Ingredient vectorization completed.\n",
      "2024-09-23 23:32:19,403:INFO:Scaling Nutritional Features...\n",
      "2024-09-23 23:32:19,439:INFO:Loading Scaler from scaler.pkl...\n",
      "2024-09-23 23:32:19,453:INFO:Nutritional features scaled.\n",
      "2024-09-23 23:32:19,456:INFO:Extracting Image Features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredient vectorization completed.\n",
      "Scaling Nutritional Features...\n",
      "Loading Scaler from scaler.pkl...\n",
      "Nutritional features scaled.\n",
      "Extracting Image Features...\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:21,027:INFO:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "2024-09-23 23:32:21,574:INFO:Loading image features from core_image_features.npy...\n",
      "2024-09-23 23:32:21,708:INFO:Loading image features from raw_image_features.npy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image features from core_image_features.npy...\n",
      "Loading image features from raw_image_features.npy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:21,852:INFO:Image feature extraction completed.\n",
      "2024-09-23 23:32:21,853:INFO:Building and Evaluating SVD Recommender System...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image feature extraction completed.\n",
      "Building and Evaluating SVD Recommender System...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:25,383:INFO:Training SVD Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:39,875:INFO:SVD model training completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:58,770:INFO:SVD Model saved to 'svd_model.pkl'.\n",
      "2024-09-23 23:32:58,771:INFO:SVD Recommender System RMSE: 0.796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Model saved to 'svd_model.pkl'.\n",
      "SVD Recommender System RMSE: 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:59,091:INFO:SVD Recommender System RMSE: 0.796\n",
      "2024-09-23 23:32:59,092:INFO:Defining Healthiness Metric...\n",
      "2024-09-23 23:32:59,095:INFO:Preparing Features and Target...\n",
      "2024-09-23 23:32:59,109:INFO:Training Samples: 27378, Validation Samples: 9126, Testing Samples: 9126\n",
      "2024-09-23 23:32:59,110:INFO:Building Keras Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Recommender System RMSE: 0.796\n",
      "Defining Healthiness Metric...\n",
      "Preparing Features and Target...\n",
      "Training Samples: 27378, Validation Samples: 9126, Testing Samples: 9126\n",
      "Building Keras Model...\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:59,112:INFO:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:59,327:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:59,335:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:59,344:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:32:59,349:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-09-23 23:32:59,358:INFO:Training Keras Model with Early Stopping...\n",
      "2024-09-23 23:32:59,370:INFO:Computed class weights: {0: 2.9337762537505356, 1: 0.6027210285311729}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Keras Model with Early Stopping...\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:00,263:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:00,271:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:00,278:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:00,300:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:00,310:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:01,639:INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 7s 37ms/step - loss: 0.1844 - accuracy: 0.8944 - val_loss: 0.0598 - val_accuracy: 0.9718\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 2s 20ms/step - loss: 0.0560 - accuracy: 0.9741 - val_loss: 0.0412 - val_accuracy: 0.9801\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.0433 - accuracy: 0.9804 - val_loss: 0.0267 - val_accuracy: 0.9875\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0374 - accuracy: 0.9839 - val_loss: 0.0297 - val_accuracy: 0.9858\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0296 - accuracy: 0.9870 - val_loss: 0.0224 - val_accuracy: 0.9894\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 2s 21ms/step - loss: 0.0271 - accuracy: 0.9887 - val_loss: 0.0300 - val_accuracy: 0.9869\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0230 - accuracy: 0.9902 - val_loss: 0.0337 - val_accuracy: 0.9853\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0195 - accuracy: 0.9913 - val_loss: 0.0133 - val_accuracy: 0.9932\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 2s 14ms/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 0.0150 - val_accuracy: 0.9940\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0191 - accuracy: 0.9922 - val_loss: 0.0217 - val_accuracy: 0.9908\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.0166 - accuracy: 0.9928 - val_loss: 0.0105 - val_accuracy: 0.9957\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 2s 14ms/step - loss: 0.0154 - accuracy: 0.9928 - val_loss: 0.0169 - val_accuracy: 0.9922\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0165 - accuracy: 0.9927 - val_loss: 0.0080 - val_accuracy: 0.9978\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0134 - accuracy: 0.9942 - val_loss: 0.0085 - val_accuracy: 0.9965\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 2s 19ms/step - loss: 0.0125 - accuracy: 0.9947 - val_loss: 0.0092 - val_accuracy: 0.9957\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0119 - accuracy: 0.9948 - val_loss: 0.0215 - val_accuracy: 0.9904\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0124 - accuracy: 0.9948 - val_loss: 0.0146 - val_accuracy: 0.9932\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0108 - accuracy: 0.9946 - val_loss: 0.0062 - val_accuracy: 0.9972\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 2s 14ms/step - loss: 0.0119 - accuracy: 0.9946 - val_loss: 0.0099 - val_accuracy: 0.9946\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0097 - accuracy: 0.9958 - val_loss: 0.0159 - val_accuracy: 0.9934\n",
      "Epoch 21/100\n",
      "107/107 [==============================] - 2s 14ms/step - loss: 0.0137 - accuracy: 0.9942 - val_loss: 0.0077 - val_accuracy: 0.9964\n",
      "Epoch 22/100\n",
      "107/107 [==============================] - 2s 14ms/step - loss: 0.0107 - accuracy: 0.9953 - val_loss: 0.0110 - val_accuracy: 0.9947\n",
      "Epoch 23/100\n",
      "107/107 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.9959 - val_loss: 0.0044 - val_accuracy: 0.9988\n",
      "Epoch 24/100\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.0088 - val_accuracy: 0.9961\n",
      "Epoch 25/100\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0105 - accuracy: 0.9955 - val_loss: 0.0119 - val_accuracy: 0.9944\n",
      "Epoch 26/100\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.0089 - accuracy: 0.9958 - val_loss: 0.0049 - val_accuracy: 0.9984\n",
      "Epoch 27/100\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0116 - accuracy: 0.9950 - val_loss: 0.0060 - val_accuracy: 0.9980\n",
      "Epoch 28/100\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0106 - accuracy: 0.9948 - val_loss: 0.0120 - val_accuracy: 0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:52,860:INFO:Model Training Completed.\n",
      "2024-09-23 23:33:52,860:INFO:Evaluating Model on Test Set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Completed.\n",
      "Evaluating Model on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:54,893:INFO:Test Accuracy: 0.9977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9977\n",
      "286/286 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:57,442:INFO:Classification Report:\n",
      "2024-09-23 23:33:57,442:INFO:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1530\n",
      "           1       1.00      1.00      1.00      7596\n",
      "\n",
      "    accuracy                           1.00      9126\n",
      "   macro avg       0.99      1.00      1.00      9126\n",
      "weighted avg       1.00      1.00      1.00      9126\n",
      "\n",
      "2024-09-23 23:33:57,446:INFO:ROC AUC Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1530\n",
      "           1       1.00      1.00      1.00      7596\n",
      "\n",
      "    accuracy                           1.00      9126\n",
      "   macro avg       0.99      1.00      1.00      9126\n",
      "weighted avg       1.00      1.00      1.00      9126\n",
      "\n",
      "ROC AUC Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:57,729:INFO:Confusion matrix plot saved as 'healthiness_confusion_matrix.png'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix plot saved as 'healthiness_confusion_matrix.png'.\n",
      "ROC curve plot saved as 'healthiness_roc_curve.png'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:57,904:INFO:ROC curve plot saved as 'healthiness_roc_curve.png'.\n",
      "2024-09-23 23:33:57,905:INFO:Plotting Training History...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Training History...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:33:58,251:INFO:Training history plot saved as 'training_history.png'.\n",
      "2024-09-23 23:33:58,252:INFO:Saving Keras Model...\n",
      "2024-09-23 23:33:58,306:INFO:Healthiness prediction model saved to healthiness_model.h5.\n",
      "2024-09-23 23:33:58,307:INFO:All tasks completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history plot saved as 'training_history.png'.\n",
      "Saving Keras Model...\n",
      "Healthiness prediction model saved to healthiness_model.h5.\n",
      "All tasks completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# NLTK for Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Surprise Library for Recommender System\n",
    "from surprise import Dataset as SurpriseDataset\n",
    "from surprise import Reader, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------- Configuration --------------------------- #\n",
    "# Define file paths\n",
    "CORE_RECIPE_PATH = 'core-data_recipe.csv'\n",
    "RAW_RECIPE_PATH = 'raw-data_recipe.csv'\n",
    "CORE_TRAIN_PATH = 'core-data-train_rating.csv'\n",
    "CORE_VALID_PATH = 'core-data-valid_rating.csv'\n",
    "CORE_TEST_PATH = 'core-data-test_rating.csv'\n",
    "RAW_INTERACTION_PATH = 'raw-data_interaction.csv'\n",
    "\n",
    "CORE_IMAGE_DIR = 'core-data-images'\n",
    "RAW_IMAGE_DIR = 'raw-data-images'\n",
    "\n",
    "# Define paths for saved features and models\n",
    "CORE_IMAGE_FEATURES_PATH = 'core_image_features.npy'\n",
    "RAW_IMAGE_FEATURES_PATH = 'raw_image_features.npy'\n",
    "TFIDF_VECTORIZER_PATH = 'tfidf_vectorizer.pkl'\n",
    "SCALER_PATH = 'scaler.pkl'\n",
    "HEALTH_MODEL_PATH = 'healthiness_model.h5'\n",
    "LE_USER_PATH = 'le_user.pkl'\n",
    "LE_RECIPE_PATH = 'le_recipe.pkl'\n",
    "SVD_MODEL_PATH = 'svd_model.pkl'\n",
    "\n",
    "# Define nutritional columns (update based on actual data)\n",
    "TARGET_NUTRIENTS = ['calories', 'protein', 'fat', 'carbohydrates', 'fiber']\n",
    "\n",
    "# Define batch size for image processing\n",
    "IMAGE_BATCH_SIZE = 32  # Adjust based on available memory\n",
    "\n",
    "# Define logging level\n",
    "LOG_LEVEL = logging.INFO\n",
    "\n",
    "# --------------------------- Logging Configuration --------------------------- #\n",
    "# Configure logging to write both to console and a log file\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --------------------------- TensorFlow GPU Configuration --------------------------- #\n",
    "def configure_tensorflow():\n",
    "    \"\"\"\n",
    "    Configure TensorFlow to use GPU if available, else fallback to CPU.\n",
    "    \"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Uncomment the following lines if you want to limit GPU memory growth\n",
    "            # for gpu in gpus:\n",
    "            #     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s) found.\")\n",
    "            logging.info(f\"{len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s) found.\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(f\"RuntimeError during GPU configuration: {e}\")\n",
    "            logging.error(f\"RuntimeError during GPU configuration: {e}\")\n",
    "    else:\n",
    "        # No GPU detected, set TensorFlow to use CPU\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "        print(\"No CUDA-capable GPU detected. TensorFlow is set to use CPU.\")\n",
    "        logging.info(\"No CUDA-capable GPU detected. TensorFlow is set to use CPU.\")\n",
    "\n",
    "# Call the configuration function\n",
    "configure_tensorflow()\n",
    "\n",
    "# --------------------------- Utility Functions --------------------------- #\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing non-alphabetic characters,\n",
    "    tokenizing, and removing stopwords.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join back to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def parse_nutritions_nested(df):\n",
    "    \"\"\"\n",
    "    Parse the 'nutritions' column assuming nested dictionaries with 'amount' keys.\n",
    "    \"\"\"\n",
    "    print(\"Parsing 'nutritions' column with nested dictionary structure...\")\n",
    "    logging.info(\"Parsing 'nutritions' column with nested dictionary structure...\")\n",
    "\n",
    "    # Function to clean the nutrition strings\n",
    "    def clean_nutrition_str(nutrition_str):\n",
    "        if isinstance(nutrition_str, str):\n",
    "            # Replace single quotes with double quotes for JSON compatibility\n",
    "            cleaned_str = nutrition_str.replace(\"u'\", \"'\")\n",
    "            return cleaned_str\n",
    "        return '{}'\n",
    "\n",
    "    # Clean the nutrition strings\n",
    "    cleaned_nutritions = df['nutritions'].apply(clean_nutrition_str)\n",
    "\n",
    "    # Safely parse using ast.literal_eval with exception handling\n",
    "    def safe_literal_eval(nutrition_str):\n",
    "        try:\n",
    "            return ast.literal_eval(nutrition_str)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error parsing nutritions: {e} for string: {nutrition_str}\")\n",
    "            return {}\n",
    "\n",
    "    nutritions_expanded = cleaned_nutritions.apply(safe_literal_eval)\n",
    "\n",
    "    # Extract only the target nutrients\n",
    "    def extract_nutrients(nutrition_dict):\n",
    "        nutrient_values = {}\n",
    "        for nutrient in TARGET_NUTRIENTS:\n",
    "            if nutrient in nutrition_dict:\n",
    "                nutrition_info = nutrition_dict[nutrient]  # Assign nutrition_info\n",
    "                if isinstance(nutrition_info, dict):\n",
    "                    amount = nutrition_info.get('amount', np.nan)\n",
    "                    if isinstance(amount, str):\n",
    "                        amount = re.sub(r'[^\\d\\.]', '', amount)\n",
    "                    nutrient_values[nutrient] = float(amount) if amount else np.nan\n",
    "                elif isinstance(nutrition_info, (int, float)):\n",
    "                    nutrient_values[nutrient] = nutrition_info\n",
    "                else:\n",
    "                    logging.warning(f\"Unexpected format for nutrient '{nutrient}'. Assigning NaN.\")\n",
    "                    nutrient_values[nutrient] = np.nan\n",
    "            else:\n",
    "                logging.warning(f\"Nutrient '{nutrient}' not found in nutrition data. Assigning NaN.\")\n",
    "                nutrient_values[nutrient] = np.nan\n",
    "        return pd.Series(nutrient_values)\n",
    "\n",
    "    nutritions_df = nutritions_expanded.apply(extract_nutrients)\n",
    "\n",
    "    # Handle missing values by filling NaNs with the mean of each nutrient\n",
    "    nutritions_df = nutritions_df.fillna(nutritions_df.mean())\n",
    "\n",
    "    # Merge the extracted nutrients back into the original DataFrame\n",
    "    df = pd.concat([df, nutritions_df], axis=1)\n",
    "\n",
    "    # Drop the original 'nutritions' column\n",
    "    df.drop('nutritions', axis=1, inplace=True)\n",
    "\n",
    "    # Ensure that nutrient columns are of float type\n",
    "    for col in TARGET_NUTRIENTS:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Fill any remaining NaNs with the mean\n",
    "    df[TARGET_NUTRIENTS] = df[TARGET_NUTRIENTS].fillna(df[TARGET_NUTRIENTS].mean())\n",
    "\n",
    "    print(\"Columns after parsing 'nutritions':\")\n",
    "    print(df.columns.tolist())\n",
    "    logging.info(f\"Columns after parsing 'nutritions': {df.columns.tolist()}\")\n",
    "\n",
    "    print(\"Parsing 'nutritions' column completed.\")\n",
    "    logging.info(\"Parsing 'nutritions' column completed.\")\n",
    "    return df\n",
    "\n",
    "def extract_image_features(recipes_df, image_dir, output_file, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extract image features using a pre-trained CNN model in batches and save them to a file.\n",
    "    Utilizes tf.data.Dataset for efficient batch processing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting image feature extraction from {image_dir}...\")\n",
    "    logging.info(f\"Starting image feature extraction from {image_dir}...\")\n",
    "\n",
    "    def get_image_paths(df, image_dir):\n",
    "        \"\"\"\n",
    "        Generate image file paths from the DataFrame.\n",
    "        \"\"\"\n",
    "        image_paths = []\n",
    "        valid_indices = []\n",
    "        for idx, recipe_id in enumerate(df['recipe_id']):\n",
    "            img_path = os.path.join(image_dir, f\"{recipe_id}.jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                image_paths.append(img_path)\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                logging.warning(f\"Image not found: {img_path}\")\n",
    "        return image_paths, valid_indices\n",
    "\n",
    "    # Create a list of image paths\n",
    "    image_paths, valid_indices = get_image_paths(recipes_df, image_dir)\n",
    "\n",
    "    # Function to load and preprocess images\n",
    "    def load_and_preprocess_image(path):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [224, 224])\n",
    "        img = preprocess_input(img)\n",
    "        return img\n",
    "\n",
    "    # Create a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for batch_images in tqdm(dataset, desc=\"Extracting Features\"):\n",
    "        batch_features = model.predict(batch_images, verbose=0)\n",
    "        features.extend(batch_features)\n",
    "\n",
    "    features = np.array(features, dtype=np.float32)\n",
    "\n",
    "    # Create a full features array with zeros for missing images\n",
    "    full_features = np.zeros((len(recipes_df), features.shape[1]), dtype=np.float32)\n",
    "    full_features[valid_indices] = features\n",
    "\n",
    "    # Save features to a numpy file\n",
    "    np.save(output_file, full_features)\n",
    "    print(f\"Image features saved to {output_file}.\")\n",
    "    logging.info(f\"Image features saved to {output_file}.\")\n",
    "\n",
    "    return full_features\n",
    "\n",
    "def load_or_extract_image_features(recipes_df, image_dir, output_file, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Load image features from a file if available; otherwise, extract and save them.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading image features from {output_file}...\")\n",
    "        logging.info(f\"Loading image features from {output_file}...\")\n",
    "        image_features = np.load(output_file).astype(np.float32)\n",
    "    else:\n",
    "        image_features = extract_image_features(recipes_df, image_dir, output_file, model, batch_size)\n",
    "    return image_features\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load all necessary CSV files into pandas DataFrames.\n",
    "    \"\"\"\n",
    "    print(\"Loading Core Recipe Data...\")\n",
    "    logging.info(\"Loading Core Recipe Data...\")\n",
    "    core_recipes = pd.read_csv(CORE_RECIPE_PATH)\n",
    "    print(f\"Core Recipes Shape: {core_recipes.shape}\")\n",
    "    logging.info(f\"Core Recipes Shape: {core_recipes.shape}\")\n",
    "\n",
    "    print(\"Loading Raw Recipe Data...\")\n",
    "    logging.info(\"Loading Raw Recipe Data...\")\n",
    "    raw_recipes = pd.read_csv(RAW_RECIPE_PATH, low_memory=False)\n",
    "    print(f\"Raw Recipes Shape: {raw_recipes.shape}\")\n",
    "    logging.info(f\"Raw Recipes Shape: {raw_recipes.shape}\")\n",
    "\n",
    "    print(\"Loading Interaction Data...\")\n",
    "    logging.info(\"Loading Interaction Data...\")\n",
    "    core_train = pd.read_csv(CORE_TRAIN_PATH)\n",
    "    core_valid = pd.read_csv(CORE_VALID_PATH)\n",
    "    core_test = pd.read_csv(CORE_TEST_PATH)\n",
    "    raw_interactions = pd.read_csv(RAW_INTERACTION_PATH)\n",
    "\n",
    "    return core_recipes, raw_recipes, core_train, core_valid, core_test, raw_interactions\n",
    "\n",
    "def preprocess_data(core_recipes, raw_recipes, core_train, core_valid, core_test, raw_interactions):\n",
    "    \"\"\"\n",
    "    Handle missing values, encode categorical variables, and preprocess text data.\n",
    "    \"\"\"\n",
    "    print(\"Handling Missing Values...\")\n",
    "    logging.info(\"Handling Missing Values...\")\n",
    "\n",
    "    # For core recipes\n",
    "    core_recipes.dropna(subset=['recipe_id', 'recipe_name', 'ingredients', 'nutritions'], inplace=True)\n",
    "    # For raw recipes\n",
    "    raw_recipes.dropna(subset=['recipe_id', 'recipe_name', 'ingredients', 'nutritions'], inplace=True)\n",
    "    # For interactions\n",
    "    core_train.dropna(inplace=True)\n",
    "    core_valid.dropna(inplace=True)\n",
    "    core_test.dropna(inplace=True)\n",
    "    raw_interactions.dropna(inplace=True)\n",
    "\n",
    "    print(\"Missing values handled.\")\n",
    "    logging.info(\"Missing values handled.\")\n",
    "\n",
    "    print(\"Encoding Categorical Variables...\")\n",
    "    logging.info(\"Encoding Categorical Variables...\")\n",
    "\n",
    "    # Convert IDs to strings\n",
    "    core_train['user_id'] = core_train['user_id'].astype(str)\n",
    "    core_train['recipe_id'] = core_train['recipe_id'].astype(str)\n",
    "    core_valid['user_id'] = core_valid['user_id'].astype(str)\n",
    "    core_valid['recipe_id'] = core_valid['recipe_id'].astype(str)\n",
    "    core_test['user_id'] = core_test['user_id'].astype(str)\n",
    "    core_test['recipe_id'] = core_test['recipe_id'].astype(str)\n",
    "    raw_interactions['user_id'] = raw_interactions['user_id'].astype(str)\n",
    "    raw_interactions['recipe_id'] = raw_interactions['recipe_id'].astype(str)\n",
    "\n",
    "    # Initialize Label Encoders\n",
    "    le_user = LabelEncoder()\n",
    "    le_recipe = LabelEncoder()\n",
    "\n",
    "    # Fit LabelEncoders on combined user and recipe IDs\n",
    "    combined_user_ids = pd.concat(\n",
    "        [\n",
    "            core_train['user_id'],\n",
    "            core_valid['user_id'],\n",
    "            core_test['user_id'],\n",
    "            raw_interactions['user_id'],\n",
    "        ]\n",
    "    ).drop_duplicates()\n",
    "    le_user.fit(combined_user_ids)\n",
    "    combined_recipe_ids = pd.concat(\n",
    "        [core_recipes['recipe_id'], raw_recipes['recipe_id']]\n",
    "    ).drop_duplicates()\n",
    "    le_recipe.fit(combined_recipe_ids)\n",
    "\n",
    "    # Encode user IDs\n",
    "    core_train['user_id_encoded'] = le_user.transform(core_train['user_id'])\n",
    "    core_valid['user_id_encoded'] = le_user.transform(core_valid['user_id'])\n",
    "    core_test['user_id_encoded'] = le_user.transform(core_test['user_id'])\n",
    "    raw_interactions['user_id_encoded'] = le_user.transform(raw_interactions['user_id'])\n",
    "\n",
    "    # Encode recipe IDs\n",
    "    core_train['recipe_id_encoded'] = le_recipe.transform(core_train['recipe_id'])\n",
    "    core_valid['recipe_id_encoded'] = le_recipe.transform(core_valid['recipe_id'])\n",
    "    core_test['recipe_id_encoded'] = le_recipe.transform(core_test['recipe_id'])\n",
    "    raw_interactions['recipe_id_encoded'] = le_recipe.transform(raw_interactions['recipe_id'])\n",
    "\n",
    "    # Encode recipe IDs in recipes DataFrames\n",
    "    core_recipes['recipe_id_encoded'] = le_recipe.transform(core_recipes['recipe_id'])\n",
    "    raw_recipes['recipe_id_encoded'] = le_recipe.transform(raw_recipes['recipe_id'])\n",
    "\n",
    "    print(\"Categorical variables encoded.\")\n",
    "    logging.info(\"Categorical variables encoded.\")\n",
    "\n",
    "    print(\"Preprocessing Text Data...\")\n",
    "    logging.info(\"Preprocessing Text Data...\")\n",
    "\n",
    "    # Download NLTK data\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # Preprocess ingredients\n",
    "    core_recipes['ingredients_clean'] = core_recipes['ingredients'].apply(preprocess_text)\n",
    "    raw_recipes['ingredients_clean'] = raw_recipes['ingredients'].apply(preprocess_text)\n",
    "\n",
    "    print(\"Text data preprocessed.\")\n",
    "    logging.info(\"Text data preprocessed.\")\n",
    "\n",
    "    # Save encoders for future use\n",
    "    joblib.dump(le_user, LE_USER_PATH)\n",
    "    joblib.dump(le_recipe, LE_RECIPE_PATH)\n",
    "    print(\"Label encoders saved.\")\n",
    "    logging.info(\"Label encoders saved.\")\n",
    "\n",
    "    # Parse 'nutritions' column\n",
    "    core_recipes = parse_nutritions_nested(core_recipes)\n",
    "    raw_recipes = parse_nutritions_nested(raw_recipes)\n",
    "\n",
    "    return (\n",
    "        core_recipes,\n",
    "        raw_recipes,\n",
    "        core_train,\n",
    "        core_valid,\n",
    "        core_test,\n",
    "        raw_interactions,\n",
    "        le_user,\n",
    "        le_recipe,\n",
    "    )\n",
    "\n",
    "def feature_engineering(core_recipes, raw_recipes):\n",
    "    \"\"\"\n",
    "    Vectorize ingredients, scale nutritional features, and extract image features.\n",
    "    \"\"\"\n",
    "    print(\"Vectorizing Ingredients with TF-IDF...\")\n",
    "    logging.info(\"Vectorizing Ingredients with TF-IDF...\")\n",
    "\n",
    "    # Check if TF-IDF vectorizer exists\n",
    "    if os.path.exists(TFIDF_VECTORIZER_PATH):\n",
    "        print(f\"Loading TF-IDF Vectorizer from {TFIDF_VECTORIZER_PATH}...\")\n",
    "        logging.info(f\"Loading TF-IDF Vectorizer from {TFIDF_VECTORIZER_PATH}...\")\n",
    "        tfidf_vectorizer = joblib.load(TFIDF_VECTORIZER_PATH)\n",
    "        ingredient_tfidf_core = tfidf_vectorizer.transform(core_recipes['ingredients_clean'])\n",
    "        ingredient_tfidf_raw = tfidf_vectorizer.transform(raw_recipes['ingredients_clean'])\n",
    "    else:\n",
    "        # Initialize and fit TF-IDF Vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        ingredient_tfidf_core = tfidf_vectorizer.fit_transform(core_recipes['ingredients_clean'])\n",
    "        ingredient_tfidf_raw = tfidf_vectorizer.transform(raw_recipes['ingredients_clean'])\n",
    "        # Save the vectorizer\n",
    "        joblib.dump(tfidf_vectorizer, TFIDF_VECTORIZER_PATH)\n",
    "        print(f\"TF-IDF Vectorizer saved to {TFIDF_VECTORIZER_PATH}.\")\n",
    "        logging.info(f\"TF-IDF Vectorizer saved to {TFIDF_VECTORIZER_PATH}.\")\n",
    "\n",
    "    print(\"Ingredient vectorization completed.\")\n",
    "    logging.info(\"Ingredient vectorization completed.\")\n",
    "\n",
    "    print(\"Scaling Nutritional Features...\")\n",
    "    logging.info(\"Scaling Nutritional Features...\")\n",
    "\n",
    "    # Convert to numeric if not already\n",
    "    core_recipes[TARGET_NUTRIENTS] = core_recipes[TARGET_NUTRIENTS].apply(pd.to_numeric, errors='coerce')\n",
    "    raw_recipes[TARGET_NUTRIENTS] = raw_recipes[TARGET_NUTRIENTS].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill missing values with mean\n",
    "    core_recipes[TARGET_NUTRIENTS] = core_recipes[TARGET_NUTRIENTS].fillna(core_recipes[TARGET_NUTRIENTS].mean())\n",
    "    raw_recipes[TARGET_NUTRIENTS] = raw_recipes[TARGET_NUTRIENTS].fillna(raw_recipes[TARGET_NUTRIENTS].mean())\n",
    "\n",
    "    # Check if scaler exists\n",
    "    if os.path.exists(SCALER_PATH):\n",
    "        print(f\"Loading Scaler from {SCALER_PATH}...\")\n",
    "        logging.info(f\"Loading Scaler from {SCALER_PATH}...\")\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "        core_recipes_scaled = scaler.transform(core_recipes[TARGET_NUTRIENTS])\n",
    "        raw_recipes_scaled = scaler.transform(raw_recipes[TARGET_NUTRIENTS])\n",
    "    else:\n",
    "        # Initialize and fit scaler\n",
    "        scaler = StandardScaler()\n",
    "        core_recipes_scaled = scaler.fit_transform(core_recipes[TARGET_NUTRIENTS])\n",
    "        raw_recipes_scaled = scaler.transform(raw_recipes[TARGET_NUTRIENTS])\n",
    "        # Save the scaler\n",
    "        joblib.dump(scaler, SCALER_PATH)\n",
    "        print(f\"Scaler saved to {SCALER_PATH}.\")\n",
    "        logging.info(f\"Scaler saved to {SCALER_PATH}.\")\n",
    "\n",
    "    # Add scaled features back to DataFrame\n",
    "    for i, col in enumerate(TARGET_NUTRIENTS):\n",
    "        core_recipes[f'{col}_scaled'] = core_recipes_scaled[:, i]\n",
    "        raw_recipes[f'{col}_scaled'] = raw_recipes_scaled[:, i]\n",
    "\n",
    "    print(\"Nutritional features scaled.\")\n",
    "    logging.info(\"Nutritional features scaled.\")\n",
    "\n",
    "    print(\"Extracting Image Features...\")\n",
    "    logging.info(\"Extracting Image Features...\")\n",
    "\n",
    "    # Load MobileNetV2 model without the top classification layers and with global average pooling\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "    # Utilize all available GPUs for the model\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            Dense(1024, activation='relu')  # Additional dense layer for feature extraction\n",
    "        ])\n",
    "\n",
    "    # Extract or load image features for core and raw datasets\n",
    "    core_image_features = load_or_extract_image_features(\n",
    "        core_recipes,\n",
    "        CORE_IMAGE_DIR,\n",
    "        CORE_IMAGE_FEATURES_PATH,\n",
    "        model,\n",
    "        batch_size=IMAGE_BATCH_SIZE\n",
    "    )\n",
    "    raw_image_features = load_or_extract_image_features(\n",
    "        raw_recipes,\n",
    "        RAW_IMAGE_DIR,\n",
    "        RAW_IMAGE_FEATURES_PATH,\n",
    "        model,\n",
    "        batch_size=IMAGE_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(\"Image feature extraction completed.\")\n",
    "    logging.info(\"Image feature extraction completed.\")\n",
    "\n",
    "    return (\n",
    "        ingredient_tfidf_core,\n",
    "        ingredient_tfidf_raw,\n",
    "        core_image_features,\n",
    "        raw_image_features,\n",
    "        tfidf_vectorizer,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "def build_and_evaluate_svd(core_train, core_valid, core_test):\n",
    "    \"\"\"\n",
    "    Combine train, validation, and test data, build the SVD model, and evaluate it.\n",
    "    \"\"\"\n",
    "    print(\"Building and Evaluating SVD Recommender System...\")\n",
    "    logging.info(\"Building and Evaluating SVD Recommender System...\")\n",
    "\n",
    "    # Combine training, validation, and test interactions\n",
    "    all_core_interactions = pd.concat([core_train, core_valid, core_test], ignore_index=True)\n",
    "\n",
    "    # Convert DataFrame into Surprise Dataset\n",
    "    reader = Reader(rating_scale=(all_core_interactions['rating'].min(), all_core_interactions['rating'].max()))\n",
    "    data = SurpriseDataset.load_from_df(\n",
    "        all_core_interactions[['user_id_encoded', 'recipe_id_encoded', 'rating']], reader\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    trainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"Training SVD Model...\")\n",
    "    logging.info(\"Training SVD Model...\")\n",
    "\n",
    "    # Initialize the SVD model\n",
    "    svd_model = SVD(\n",
    "        n_factors=100,\n",
    "        n_epochs=20,\n",
    "        lr_all=0.005,\n",
    "        reg_all=0.4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model on trainset\n",
    "    svd_model.fit(trainset)\n",
    "    logging.info(\"SVD model training completed.\")\n",
    "\n",
    "    # Test the model on the test set\n",
    "    predictions = svd_model.test(testset)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "    # Save the SVD model\n",
    "    joblib.dump(svd_model, SVD_MODEL_PATH)\n",
    "    print(f\"SVD Model saved to '{SVD_MODEL_PATH}'.\")\n",
    "    logging.info(f\"SVD Model saved to '{SVD_MODEL_PATH}'.\")\n",
    "\n",
    "    print(f\"SVD Recommender System RMSE: {rmse:.3f}\")\n",
    "    logging.info(f\"SVD Recommender System RMSE: {rmse:.3f}\")\n",
    "\n",
    "    return svd_model, rmse\n",
    "\n",
    "def build_healthiness_model(core_recipes):\n",
    "    \"\"\"\n",
    "    Build and train the Keras-based neural network for healthiness prediction.\n",
    "    \"\"\"\n",
    "    print(\"Defining Healthiness Metric...\")\n",
    "    logging.info(\"Defining Healthiness Metric...\")\n",
    "    # Define target variable (healthy if calories < 500)\n",
    "    core_recipes['is_healthy'] = (core_recipes['calories'] < 500).astype(int)\n",
    "\n",
    "    print(\"Preparing Features and Target...\")\n",
    "    logging.info(\"Preparing Features and Target...\")\n",
    "\n",
    "    # Features and target\n",
    "    feature_cols = [f'{col}_scaled' for col in TARGET_NUTRIENTS]\n",
    "    X = core_recipes[feature_cols].values.astype(np.float32)\n",
    "    y = core_recipes['is_healthy'].values\n",
    "\n",
    "    # Split into training, validation, and testing sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "    print(f\"Training Samples: {X_train.shape[0]}, Validation Samples: {X_val.shape[0]}, Testing Samples: {X_test.shape[0]}\")\n",
    "    logging.info(f\"Training Samples: {X_train.shape[0]}, Validation Samples: {X_val.shape[0]}, Testing Samples: {X_test.shape[0]}\")\n",
    "\n",
    "    print(\"Building Keras Model...\")\n",
    "    logging.info(\"Building Keras Model...\")\n",
    "\n",
    "    # Utilize all available GPUs for the model\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        # Define the neural network architecture\n",
    "        model = Sequential([\n",
    "            InputLayer(input_shape=(X_train.shape[1],)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.4),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    print(\"Training Keras Model with Early Stopping...\")\n",
    "    logging.info(\"Training Keras Model with Early Stopping...\")\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Calculate class weights to handle class imbalance if present\n",
    "    class_weights_dict = {}\n",
    "    if len(np.unique(y_train)) > 1:\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights_dict = dict(enumerate(class_weights))\n",
    "        logging.info(f\"Computed class weights: {class_weights_dict}\")\n",
    "    else:\n",
    "        logging.info(\"Only one class present in y_train. No class weights computed.\")\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        batch_size=256,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights_dict if class_weights_dict else None,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Model Training Completed.\")\n",
    "    logging.info(\"Model Training Completed.\")\n",
    "\n",
    "    print(\"Evaluating Model on Test Set...\")\n",
    "    logging.info(\"Evaluating Model on Test Set...\")\n",
    "    # Evaluate on test set\n",
    "    loss, accuracy_val = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {accuracy_val:.4f}\")\n",
    "    logging.info(f\"Test Accuracy: {accuracy_val:.4f}\")\n",
    "\n",
    "    # Generate classification report\n",
    "    y_pred_prob = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    logging.info(\"Classification Report:\")\n",
    "    logging.info(report)\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    logging.info(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Unhealthy', 'Healthy'],\n",
    "        yticklabels=['Unhealthy', 'Healthy']\n",
    "    )\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('healthiness_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    print(\"Confusion matrix plot saved as 'healthiness_confusion_matrix.png'.\")\n",
    "    logging.info(\"Confusion matrix plot saved as 'healthiness_confusion_matrix.png'.\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.4f})')\n",
    "    plt.plot([0,1], [0,1], 'k--')  # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Healthiness Prediction')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig('healthiness_roc_curve.png')\n",
    "    plt.close()\n",
    "    print(\"ROC curve plot saved as 'healthiness_roc_curve.png'.\")\n",
    "    logging.info(\"ROC curve plot saved as 'healthiness_roc_curve.png'.\")\n",
    "\n",
    "    # Plot Training History\n",
    "    print(\"Plotting Training History...\")\n",
    "    logging.info(\"Plotting Training History...\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    print(\"Training history plot saved as 'training_history.png'.\")\n",
    "    logging.info(\"Training history plot saved as 'training_history.png'.\")\n",
    "\n",
    "    print(\"Saving Keras Model...\")\n",
    "    logging.info(\"Saving Keras Model...\")\n",
    "    # Save the trained model\n",
    "    model.save(HEALTH_MODEL_PATH)\n",
    "    print(f\"Healthiness prediction model saved to {HEALTH_MODEL_PATH}.\")\n",
    "    logging.info(f\"Healthiness prediction model saved to {HEALTH_MODEL_PATH}.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# --------------------------- Main Execution --------------------------- #\n",
    "def main():\n",
    "    try:\n",
    "        # Step 1: Load Data\n",
    "        core_recipes, raw_recipes, core_train, core_valid, core_test, raw_interactions = load_data()\n",
    "\n",
    "        # Step 2: Preprocess Data\n",
    "        (\n",
    "            core_recipes,\n",
    "            raw_recipes,\n",
    "            core_train,\n",
    "            core_valid,\n",
    "            core_test,\n",
    "            raw_interactions,\n",
    "            le_user,\n",
    "            le_recipe,\n",
    "        ) = preprocess_data(core_recipes, raw_recipes, core_train, core_valid, core_test, raw_interactions)\n",
    "\n",
    "        # Step 3: Feature Engineering\n",
    "        (\n",
    "            ingredient_tfidf_core,\n",
    "            ingredient_tfidf_raw,\n",
    "            core_image_features,\n",
    "            raw_image_features,\n",
    "            tfidf_vectorizer,\n",
    "            scaler,\n",
    "        ) = feature_engineering(core_recipes, raw_recipes)\n",
    "\n",
    "        # Step 4: Build and Evaluate Recommender System (Using SVD)\n",
    "        svd_model, svd_rmse = build_and_evaluate_svd(core_train, core_valid, core_test)\n",
    "\n",
    "        print(f\"SVD Recommender System RMSE: {svd_rmse:.3f}\")\n",
    "        logging.info(f\"SVD Recommender System RMSE: {svd_rmse:.3f}\")\n",
    "\n",
    "        # Step 5: Build Healthiness Prediction Model\n",
    "        health_model = build_healthiness_model(core_recipes)\n",
    "\n",
    "        print(\"All tasks completed successfully.\")\n",
    "        logging.info(\"All tasks completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        logging.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
